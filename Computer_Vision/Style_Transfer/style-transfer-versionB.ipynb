{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install torch_snippets\n\n# TODO: Import relevant packages\nfrom torch_snippets import *\nfrom torchvision import transforms\nfrom torch.nn import functional as F\nfrom torchvision.models import vgg19","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-24T06:04:55.249872Z","iopub.execute_input":"2022-02-24T06:04:55.250163Z","iopub.status.idle":"2022-02-24T06:05:10.955724Z","shell.execute_reply.started":"2022-02-24T06:04:55.250132Z","shell.execute_reply":"2022-02-24T06:05:10.954881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shifted to the cuda\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:10.957677Z","iopub.execute_input":"2022-02-24T06:05:10.957925Z","iopub.status.idle":"2022-02-24T06:05:10.963796Z","shell.execute_reply.started":"2022-02-24T06:05:10.957892Z","shell.execute_reply":"2022-02-24T06:05:10.962161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Define the functions to prepossess and postprocessing the data\npre_process = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.Lambda(lambda x: x.mul_(255))\n])\n\npost_process = transforms.Compose([\n    transforms.Lambda(lambda x: x.mul_(1./255)),\n    transforms.Normalize(mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225], std=[1/0.229, 1/0.224, 1/0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:10.966504Z","iopub.execute_input":"2022-02-24T06:05:10.966704Z","iopub.status.idle":"2022-02-24T06:05:10.974743Z","shell.execute_reply.started":"2022-02-24T06:05:10.966682Z","shell.execute_reply":"2022-02-24T06:05:10.974115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Define GramMatrix module\nclass GramMatrix(nn.Module):\n    def forward(self, input):\n        b, c, h, w = input.size()\n        feat = input.view(b, c, h * w)\n        G = feat @ feat.transpose(1, 2)  # It multiplied (inner product) by transpose itself\n        G.div_(h * w)\n        return G","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:10.976939Z","iopub.execute_input":"2022-02-24T06:05:10.977307Z","iopub.status.idle":"2022-02-24T06:05:10.983823Z","shell.execute_reply.started":"2022-02-24T06:05:10.977271Z","shell.execute_reply":"2022-02-24T06:05:10.982733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: GramMatrix corresponding to the MSELoss, GramMSELoss\nclass GramMSELoss(nn.Module):\n    def forward(self, input, target):\n        out = F.mse_loss(GramMatrix()(input), target)\n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:10.985052Z","iopub.execute_input":"2022-02-24T06:05:10.985286Z","iopub.status.idle":"2022-02-24T06:05:10.992267Z","shell.execute_reply.started":"2022-02-24T06:05:10.985251Z","shell.execute_reply":"2022-02-24T06:05:10.991528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we have the gram vectors for both feature sets, it is important that\nthey match as closely as possible, and hence the `mse_loss`.","metadata":{}},{"cell_type":"code","source":"# TODO: Define the model class, vgg19_modified\nclass vgg19_modified(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Extract features\n        features = list(vgg19(pretrained=True).features)\n        self.features = nn.ModuleList(features).eval()\n\n    # TODO: define the forward method\n    def forward(self, x, layers=[]):\n        order = np.argsort(layers)\n        _results, results = [], []\n        for idx, model in enumerate(self.features):\n            x = model(x)\n            if idx in layers: _results.append(x)\n        for o in order: results.append(_results[o])\n        return results if layers is not [] else x","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:10.993573Z","iopub.execute_input":"2022-02-24T06:05:10.993843Z","iopub.status.idle":"2022-02-24T06:05:11.003749Z","shell.execute_reply.started":"2022-02-24T06:05:10.99379Z","shell.execute_reply":"2022-02-24T06:05:11.002874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Define the object model\nvgg = vgg19_modified().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:11.005079Z","iopub.execute_input":"2022-02-24T06:05:11.005404Z","iopub.status.idle":"2022-02-24T06:05:38.034283Z","shell.execute_reply.started":"2022-02-24T06:05:11.005371Z","shell.execute_reply":"2022-02-24T06:05:38.03353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Import the content and style images:","metadata":{}},{"cell_type":"code","source":"# TODO: improt the content and style images\n# !wget https://www.dropbox.com/s/z1y0fy2r6z6m6py/60.jpg\n# !wget https://www.dropbox.com/s/1svdliljyo0a98v/style_image.png\n!wget https://raw.githubusercontent.com/bensains1/fast-style-transfer-master/master/examples/content/chicago.jpg\n!wget https://raw.githubusercontent.com/bensains1/fast-style-transfer-master/master/examples/style/the_shipwreck_of_the_minotaur.jpg\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:38.0356Z","iopub.execute_input":"2022-02-24T06:05:38.035839Z","iopub.status.idle":"2022-02-24T06:05:39.655721Z","shell.execute_reply.started":"2022-02-24T06:05:38.035807Z","shell.execute_reply":"2022-02-24T06:05:39.654945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Make sure that the images are resized to be of the same shape, 512 x 512 x 3\nimgs = [Image.open(path).resize((512, 412)).convert('RGB') for path in ['./the_shipwreck_of_the_minotaur.jpg', './chicago.jpg']]\nstyle_image, content_image = [pre_process(img).to(device)[None] for img in imgs]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:39.658331Z","iopub.execute_input":"2022-02-24T06:05:39.658853Z","iopub.status.idle":"2022-02-24T06:05:39.792839Z","shell.execute_reply.started":"2022-02-24T06:05:39.658814Z","shell.execute_reply":"2022-02-24T06:05:39.792119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Specify that the content image is to modified with requires_grad = True\nopt_img = content_image.data.clone()\nopt_img.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:39.795577Z","iopub.execute_input":"2022-02-24T06:05:39.795819Z","iopub.status.idle":"2022-02-24T06:05:39.802225Z","shell.execute_reply.started":"2022-02-24T06:05:39.795787Z","shell.execute_reply":"2022-02-24T06:05:39.801523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Specify the layers that define content loss and style loss, that is, which intermediate VGG layers we are using, to compare gram matrices for style and raw feature vectors for content**","metadata":{}},{"cell_type":"code","source":"style_layers = [0, 5, 10, 19, 28]\ncontent_layers = [21]\nloss_layers = style_layers + content_layers","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:39.80529Z","iopub.execute_input":"2022-02-24T06:05:39.805497Z","iopub.status.idle":"2022-02-24T06:05:39.811715Z","shell.execute_reply.started":"2022-02-24T06:05:39.805454Z","shell.execute_reply":"2022-02-24T06:05:39.81091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Define the loss function for content and style loss values:\nloss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\nloss_fns = [loss_fn.to(device) for loss_fn in loss_fns]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:39.813069Z","iopub.execute_input":"2022-02-24T06:05:39.813393Z","iopub.status.idle":"2022-02-24T06:05:39.821738Z","shell.execute_reply.started":"2022-02-24T06:05:39.813357Z","shell.execute_reply":"2022-02-24T06:05:39.821024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Define the weightage associated with content and style loss\nstyle_weights = [1000/n**2 for n in [64, 128, 256, 512, 512]]\ncontent_weights = [1]\nweights = style_weights + content_weights","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:39.823083Z","iopub.execute_input":"2022-02-24T06:05:39.823596Z","iopub.status.idle":"2022-02-24T06:05:39.833353Z","shell.execute_reply.started":"2022-02-24T06:05:39.823558Z","shell.execute_reply":"2022-02-24T06:05:39.832627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***We need to manipulate our image such that the style of the target image\nresembles`style_image` as much as possible. Hence we compute the\n`style_targets` values of `style_image` by computing GramMatrix of\nfeatures obtained from a few chosen layers of VGG. Since the overall\ncontent should be preserved, we choose the `content_layer` variable at\nwhich we compute the raw features from VGG:***","metadata":{}},{"cell_type":"code","source":"style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\ncontent_targets = [A.detach() for A in vgg(content_image, content_layers)]\ntargets = style_targets + content_targets","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:39.834774Z","iopub.execute_input":"2022-02-24T06:05:39.835328Z","iopub.status.idle":"2022-02-24T06:05:45.282437Z","shell.execute_reply.started":"2022-02-24T06:05:39.83529Z","shell.execute_reply":"2022-02-24T06:05:45.281645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the optimizer and the number of iterations ***(max_iters).*** Even\nthough we could have used Adam or any other optimizer, **LBFGS** is an\noptimizer that has been observed to work best in deterministic scenarios.\nAdditionally, since we are dealing with exactly one image, there is nothing\nrandom. Many experiments have revealed that **LBFGS** converges faster and\nto lower losses in neural transfer settings, so we will use this optimizer:","metadata":{}},{"cell_type":"code","source":"max_iters = 6000\noptimizer = optim.LBFGS([opt_img], lr = 0.1)\nlog = Report(max_iters)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:45.283844Z","iopub.execute_input":"2022-02-24T06:05:45.284105Z","iopub.status.idle":"2022-02-24T06:05:45.289087Z","shell.execute_reply.started":"2022-02-24T06:05:45.284071Z","shell.execute_reply":"2022-02-24T06:05:45.28837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Perform the optimization. In deterministic scenarios where we are iterating\non the same tensor again and again, we can wrap the optimizer step as a\nfunction with zero arguments and repeatedly call it, as shown here:*","metadata":{}},{"cell_type":"code","source":"iters = 0\nwhile iters < max_iters:\n    def closure():\n        global iters\n        iters += 1\n        optimizer.zero_grad()\n        out = vgg(opt_img, loss_layers)\n        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n        loss = sum(layer_losses)\n        loss.backward()\n        log.record(pos=iters, loss=loss, end='\\r')\n        return loss\n    optimizer.step(closure)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:05:45.290369Z","iopub.execute_input":"2022-02-24T06:05:45.290788Z","iopub.status.idle":"2022-02-24T06:19:41.99955Z","shell.execute_reply.started":"2022-02-24T06:05:45.290752Z","shell.execute_reply":"2022-02-24T06:19:41.99853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the variation in the loss:\nlog.plot(log=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:19:42.000545Z","iopub.status.idle":"2022-02-24T06:19:42.001799Z","shell.execute_reply.started":"2022-02-24T06:19:42.001542Z","shell.execute_reply":"2022-02-24T06:19:42.00157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the image with the combination of content and style images:\nwith torch.no_grad():\n    out_img = post_process(opt_img[0]).permute(1,2,0)\nshow(out_img)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:19:42.003074Z","iopub.status.idle":"2022-02-24T06:19:42.003695Z","shell.execute_reply.started":"2022-02-24T06:19:42.00345Z","shell.execute_reply":"2022-02-24T06:19:42.003473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}