{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch import optim\nfrom PIL import Image\nfrom torchvision import transforms, models\nfrom torchvision.utils import save_image\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-06T09:58:57.160453Z","iopub.execute_input":"2022-03-06T09:58:57.161056Z","iopub.status.idle":"2022-03-06T09:58:57.166151Z","shell.execute_reply.started":"2022-03-06T09:58:57.161017Z","shell.execute_reply":"2022-03-06T09:58:57.165199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre_process = transforms.Compose([\n#     transforms.Resize((512, 712)),\n#     transforms.ToTensor(),\n    \n# ])","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:58:57.169484Z","iopub.execute_input":"2022-03-06T09:58:57.170597Z","iopub.status.idle":"2022-03-06T09:58:57.176901Z","shell.execute_reply.started":"2022-03-06T09:58:57.170337Z","shell.execute_reply":"2022-03-06T09:58:57.17618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class vgg_modified(nn.Module):\n    def __init__(self):\n        super(vgg_modified, self).__init__()\n        self.chosen_features = ['0', '5', '10', '19', '28']\n        self.model = models.vgg19(pretrained=True).features[:29]\n\n    def forward(self, x):\n        features = []\n        for layer_num, layer in enumerate(self.model):\n            x = layer(x)\n            if str(layer_num) in self.chosen_features:\n                features.append(x)\n\n        return features","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:58:57.179096Z","iopub.execute_input":"2022-03-06T09:58:57.180111Z","iopub.status.idle":"2022-03-06T09:58:57.187663Z","shell.execute_reply.started":"2022-03-06T09:58:57.180006Z","shell.execute_reply":"2022-03-06T09:58:57.186983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def load_image(image_name):\n#     image = Image.open(image_name)\n#     image = pre_process(image).unsqueeze(0)\n#     return image.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:58:57.189224Z","iopub.execute_input":"2022-03-06T09:58:57.189813Z","iopub.status.idle":"2022-03-06T09:58:57.200639Z","shell.execute_reply.started":"2022-03-06T09:58:57.189767Z","shell.execute_reply":"2022-03-06T09:58:57.199668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In the paper, if we take more than 500 x 500 size it would take more than a hours. \n\ndef load_image(img_path, max_size = 612, shape = None):\n    ''' Load in and transform an image, making sure the image\n    is <= 400 pixels in the x-y dims.'''\n    if 'http' in img_path:\n        response = requests.get(img_path)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n        \n    else:\n        image = Image.open(img_path).convert('RGB')\n        \n    # Load images will be slow down processing\n    \n    if max(image.size) > max_size:\n        size = max_size\n    else:\n        size = max(image.size)\n        \n    if shape is not None:\n        size = shape\n\n\n#     in_transform = transforms.Compose([\n#                         transforms.Resize((612, 512)),\n#                         transforms.ToTensor(),\n#                         transforms.Normalize((0.485, 0.456, 0.406), \n#                                              (0.229, 0.224, 0.225))])\n    \n    in_transform = transforms.Compose([\n                    transforms.Resize((712, 512)),\n                    transforms.ToTensor()])\n    \n    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n    image = in_transform(image)[:3, :, :].unsqueeze(0)\n    \n    return image.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:58:57.204002Z","iopub.execute_input":"2022-03-06T09:58:57.205854Z","iopub.status.idle":"2022-03-06T09:58:57.214059Z","shell.execute_reply.started":"2022-03-06T09:58:57.205822Z","shell.execute_reply":"2022-03-06T09:58:57.213304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !wget https://www.dropbox.com/s/z1y0fy2r6z6m6py/60.jpg\n# !wget https://www.dropbox.com/s/1svdliljyo0a98v/style_image.png\n# !wget https://raw.githubusercontent.com/bensains1/fast-style-transfer-master/master/examples/content/chicago.jpg\n# !wget https://raw.githubusercontent.com/bensains1/fast-style-transfer-master/master/examples/style/the_shipwreck_of_the_minotaur.jpg\n# !wget https://raw.githubusercontent.com/bensains1/fast-style-transfer-master/master/examples/style/udnie.jpg\n# !wget https://raw.githubusercontent.com/bensains1/fast-style-transfer-master/master/examples/style/the_scream.jpg","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:58:57.216078Z","iopub.execute_input":"2022-03-06T09:58:57.21678Z","iopub.status.idle":"2022-03-06T09:58:57.223924Z","shell.execute_reply.started":"2022-03-06T09:58:57.216719Z","shell.execute_reply":"2022-03-06T09:58:57.223141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function for un-normalizing an image \n# and converting it from a Tensor image to a NumPy image for display\ndef im_convert(tensor):\n    \"\"\" Display a tensor as an image. \"\"\"\n    \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image.transpose(1,2,0)\n    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))  # Denormalize\n    image = image.clip(0, 1)\n\n    return image;","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:58:57.226285Z","iopub.execute_input":"2022-03-06T09:58:57.227Z","iopub.status.idle":"2022-03-06T09:58:57.237279Z","shell.execute_reply.started":"2022-03-06T09:58:57.22696Z","shell.execute_reply":"2022-03-06T09:58:57.23636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ncontent_img = load_image('../input/imagedataset/fub2.jpg')\nstyle_img = load_image('../input/imagedataset/hockney.jpg')\n\nmodel = vgg_modified().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:58:57.238586Z","iopub.execute_input":"2022-03-06T09:58:57.239565Z","iopub.status.idle":"2022-03-06T09:58:58.999174Z","shell.execute_reply.started":"2022-03-06T09:58:57.239524Z","shell.execute_reply":"2022-03-06T09:58:58.998417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the images\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n# content and style ims side-by-side\nax1.imshow(content_img.cpu().detach().numpy().squeeze().transpose(1,2,0))  # Run without normalize \nax2.imshow(style_img.cpu().detach().numpy().squeeze().transpose(1,2,0))    # Run without normalize\n# ax1.imshow(im_convert(content_img))   # Run with normalize\n# ax2.imshow(im_convert(style_img))     # Run with normalize","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:58:59.001202Z","iopub.execute_input":"2022-03-06T09:58:59.001618Z","iopub.status.idle":"2022-03-06T09:59:00.020115Z","shell.execute_reply.started":"2022-03-06T09:58:59.001582Z","shell.execute_reply":"2022-03-06T09:59:00.019258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generated_img = torch.randn(content_img.shape, device = device, requires_grad=True)\ngenerated_img = content_img.clone()\ngenerated_img.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:59:00.02227Z","iopub.execute_input":"2022-03-06T09:59:00.022717Z","iopub.status.idle":"2022-03-06T09:59:00.030673Z","shell.execute_reply.started":"2022-03-06T09:59:00.022682Z","shell.execute_reply":"2022-03-06T09:59:00.03009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO: Hyperparameters\ntotal_steps = 15000\nlearning_rate = 0.001\nalpha = 1\nbeta = 0.001\noptimizer = optim.Adam([generated_img], lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:59:00.031863Z","iopub.execute_input":"2022-03-06T09:59:00.033409Z","iopub.status.idle":"2022-03-06T09:59:00.237821Z","shell.execute_reply.started":"2022-03-06T09:59:00.033364Z","shell.execute_reply":"2022-03-06T09:59:00.236856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport time\nstart = time.time()\nfor step in tqdm(range(total_steps)):\n    generated_features = model(generated_img)\n    content_features = model(content_img)\n    style_features = model(style_img)\n\n    style_loss = content_loss = 0\n\n    for gen_feature, content_feature, style_feature in zip(generated_features, content_features, style_features):\n        batch_size, channel, height, width = gen_feature.shape\n        content_loss += torch.mean((gen_feature - content_feature) ** 2)\n\n        # Compute Gram Matrix\n        G = gen_feature.view(channel, height * width).mm\\\n            (gen_feature.view(channel, height * width).t())\n\n        S = style_feature.view(channel, height * width).mm\\\n            (style_feature.view(channel, height * width).t())\n\n        style_loss += torch.mean((G - S) ** 2)\n        \n    end = time.time()\n\n    total_loss = alpha * content_loss + beta * style_loss\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n\n    if step % 400 == 0:\n        print(\"EPOCH: {}/{} \\tTotal Loss: {:2f} \\tTime Elapsed: {:4f}\".format(step, total_steps, total_loss, end - start))\n        plt.imshow(generated_img.cpu().detach().numpy().squeeze().transpose(1,2,0))  # image display\n        save_image(generated_img, 'generated.png')\n        plt.show()\n        ","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:59:00.23975Z","iopub.execute_input":"2022-03-06T09:59:00.240479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(generated_img.cpu().detach().numpy().squeeze().transpose(1,2,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the image with the combination of content and style images:\n# with torch.no_grad():\n#     out_img = post_process(generated_img[0]).permute(1,2,0)\n# show(out_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}