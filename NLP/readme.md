# Natural Language Processing
<p> Natural language processing strives to build machines that understand and respond to text or voice data—and respond with text or speech of their own—in much the same way humans do. </p>

## Character wise language model using RNN:
To produce fresh text data from enormous chunks of text, I constructed a character-wise language model using a recurrent neural network (RNN). For this model, I used Prof. Karpathy's RNN work. I had a lot of trouble implementing and grasping the sophisticated formalism that was used to process them sequentially. First, I encode each character into a vector (one-hot encoding), then feed it one at a time through an RNN, which generates a probability for each letter. Essentially, it creates text by sampling the next character from the top k, most likely depending on the probability distribution (Softmax classifier) of the last character in the current sequence. Mini batches were used to train the model.


## Sentiment analysis:
I created a model that can read a piece of text and determine if the emotion is favourable or negative. I trained the model using an IMDB dataset of movie reviews tagged as "good" or "negative." Because this is text data in the form of words in a series, I conducted data preparation such as tokenizing the words, cleaning, and padding the data. The data was then sent into a Recurrent Neural Network with two Long Short-Term Memory (LSTM) hidden layers to produce a model that takes into account not only the individual words but also the sequence in which they appear. Then, to prevent overfitting and construct a well-generalized model for predicting whether text is positive or negative, a dropout layer is utilised. The model obtained 81 percent accuracy on the test set and was tested with new viewpoints and content.
